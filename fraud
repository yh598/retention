import matplotlib.pyplot as plt

# View class distribution
fraud_counts = df['fraud'].value_counts()
print(fraud_counts)

# Plot it
plt.figure(figsize=(6,4))
fraud_counts.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title("Fraud vs. Non-Fraud Count")
plt.xlabel("Fraud")
plt.ylabel("Number of Records")
plt.xticks(ticks=[0, 1], labels=['Non-Fraud (0)', 'Fraud (1)'], rotation=0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

# Load data
df = pd.read_csv('training_data.csv')

# Drop columns that shouldn't be features
X = df.drop(columns=['fraud', 'Unnamed: 0'])  # drop ID or unnamed columns if present
y = df['fraud']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Optional: scale features if needed
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Random Forest
clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
clf.fit(X_train_scaled, y_train)

y_pred = clf.predict(X_test_scaled)

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# --- Imports ---
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import resample
import matplotlib.pyplot as plt

# --- Step 1: Load data ---
df = pd.read_csv('training_data.csv')

# Drop unnamed column if present
df.drop(columns=['Unnamed: 0'], errors='ignore', inplace=True)

# --- Step 2: Explore class imbalance ---
fraud_counts = df['fraud'].value_counts()
print("Original class distribution:")
print(fraud_counts)

plt.figure(figsize=(6,4))
fraud_counts.plot(kind='bar', color=['skyblue', 'salmon'])
plt.title("Fraud vs. Non-Fraud Count")
plt.xticks(ticks=[0, 1], labels=['Non-Fraud (0)', 'Fraud (1)'], rotation=0)
plt.ylabel("Number of Records")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# --- Step 3: Balance the dataset (undersample majority class) ---
df_majority = df[df.fraud == 0]
df_minority = df[df.fraud == 1]

df_majority_downsampled = resample(
    df_majority,
    replace=False,
    n_samples=len(df_minority) * 5,  # 5:1 ratio (can adjust to 1:1 for full balance)
    random_state=42
)

df_balanced = pd.concat([df_majority_downsampled, df_minority])
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

print("\nBalanced class distribution:")
print(df_balanced['fraud'].value_counts())

# --- Step 4: Feature matrix & labels ---
X = df_balanced.drop(columns=['fraud'])
y = df_balanced['fraud']

# --- Step 5: Train/test split ---
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# --- Step 6: Feature scaling ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- Step 7: Train model ---
clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
clf.fit(X_train_scaled, y_train)

# --- Step 8: Evaluate model ---
y_pred = clf.predict(X_test_scaled)

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# Plot top 20 feature importances
importances = clf.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)[::-1][:20]  # top 20

plt.figure(figsize=(10,6))
plt.barh(range(len(indices)), importances[indices][::-1])
plt.yticks(range(len(indices)), [feature_names[i] for i in indices][::-1])
plt.xlabel("Feature Importance")
plt.title("Top 20 Important Features")
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Get probabilities
y_proba = clf.predict_proba(X_test_scaled)[:, 1]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc_score = roc_auc_score(y_test, y_proba)

# Plot ROC curve
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.4f}")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class
y_proba = clf.predict_proba(X_test_scaled)[:, 1]

# Calculate precision and recall
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
avg_precision = average_precision_score(y_test, y_proba)

# Plot Precision-Recall curve
plt.figure(figsize=(6, 6))
plt.plot(recall, precision, label=f'PR curve (AP = {avg_precision:.4f})', color='blue')
plt.axhline(y=0.2, color='red', linestyle='--', label='Baseline')

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend(loc="upper right")
plt.grid(True)
plt.tight_layout()
plt.show()

import pickle
import datetime
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    precision_recall_curve,
    classification_report
)

# === Load production model ===
with open('destro_digital.pkl', 'rb') as f:
    prod_model = pickle.load(f)

# === Train current model ===
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# === Predict probabilities ===
prod_probs = prod_model.predict_proba(X_test)[:, 1]
curr_probs = clf.predict_proba(X_test)[:, 1]

# === Compute metrics ===
prod_auc = roc_auc_score(y_test, prod_probs)
curr_auc = roc_auc_score(y_test, curr_probs)

prod_ap = average_precision_score(y_test, prod_probs)
curr_ap = average_precision_score(y_test, curr_probs)

print(f"Production Model AUC: {prod_auc:.4f}, AP: {prod_ap:.4f}")
print(f"Current Model AUC:    {curr_auc:.4f}, AP: {curr_ap:.4f}")

# === Optional: Classification Report ===
print("\nClassification Report (Current Model):")
print(classification_report(y_test, clf.predict(X_test)))

# === Plot Precision-Recall Curves ===
precision_prod, recall_prod, _ = precision_recall_curve(y_test, prod_probs)
precision_curr, recall_curr, _ = precision_recall_curve(y_test, curr_probs)

plt.figure(figsize=(7, 5))
plt.plot(recall_prod, precision_prod, label=f'Production (AP={prod_ap:.3f})', color='orange')
plt.plot(recall_curr, precision_curr, label=f'Current (AP={curr_ap:.3f})', color='blue')
plt.axhline(y=0.2, linestyle='--', color='red', label='Baseline')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Comparison')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# === Save current model with today's date ===
today_str = datetime.datetime.today().strftime('%Y-%m-%d')
model_filename = f'current_model_{today_str}.pkl'
with open(model_filename, 'wb') as f:
    pickle.dump(clf, f)

print(f"\nCurrent model saved to: {model_filename}")

conda create -n skl_compat python=3.8 scikit-learn=0.24.2 -y
conda activate skl_compat

import pickle
import joblib

# Load with old sklearn
with open('destro_digital.pkl', 'rb') as f:
    model = pickle.load(f)

# Save using joblib
joblib.dump(model, 'destro_digital_compat.joblib')

import joblib

# Load the model
model = joblib.load('destro_digital_compat.joblib')

# Predict as usual (e.g., on test data)
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # If you need probabilities

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix,
    precision_recall_curve, average_precision_score, recall_score
)
import xgboost as xgb

# Step 1: Keep all fraud, sample non-fraud to balance (or partially balance)
df_fraud = df[df['fraud'] == 1]
df_nonfraud = df[df['fraud'] == 0].sample(n=len(df_fraud)*5, random_state=42)
df_balanced = pd.concat([df_fraud, df_nonfraud]).sample(frac=1, random_state=42).reset_index(drop=True)

# Step 2: Features and labels
X = df_balanced.drop(columns=['fraud', 'usaa_party_pd_id'])  # drop label + ID
y = df_balanced['fraud']

# Step 3: Split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Step 4: Train XGBoost model
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='aucpr',
    scale_pos_weight=scale_pos_weight,
    use_label_encoder=False,
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)
xgb_model.fit(X_train, y_train)

# Step 5: Predict probabilities
y_proba = xgb_model.predict_proba(X_test)[:, 1]

# Step 6: Optimize threshold for recall
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
best_idx = np.argmax(recall)
best_threshold = thresholds[best_idx]
print(f"Best recall: {recall[best_idx]:.4f} at threshold: {best_threshold:.4f}")

# Step 7: Apply threshold
y_pred = (y_proba >= best_threshold).astype(int)

# Step 8: Evaluate
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

print(f"\nAP (PR-AUC): {average_precision_score(y_test, y_proba):.4f}")
print(f"Recall (at best threshold): {recall_score(y_test, y_pred):.4f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix,
    precision_recall_curve, average_precision_score, recall_score
)
import lightgbm as lgb

# Step 1: Keep all fraud, sample non-fraud
df_fraud = df[df['fraud'] == 1]
df_nonfraud = df[df['fraud'] == 0].sample(n=len(df_fraud)*5, random_state=42)
df_balanced = pd.concat([df_fraud, df_nonfraud]).sample(frac=1, random_state=42).reset_index(drop=True)

# Step 2: Features and labels
X = df_balanced.drop(columns=['fraud', 'usaa_party_pd_id'])
y = df_balanced['fraud']

# Step 3: Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Step 4: Train LightGBM model
lgb_model = lgb.LGBMClassifier(
    objective='binary',
    class_weight='balanced',
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)
lgb_model.fit(X_train, y_train)

# Step 5: Predict probabilities
y_proba = lgb_model.predict_proba(X_test)[:, 1]

# Step 6: Optimize threshold for recall
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
best_idx = np.argmax(recall)
best_threshold = thresholds[best_idx]
print(f"Best recall: {recall[best_idx]:.4f} at threshold: {best_threshold:.4f}")

# Step 7: Apply threshold
y_pred = (y_proba >= best_threshold).astype(int)

# Step 8: Evaluate
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

print(f"\nAP (PR-AUC): {average_precision_score(y_test, y_proba):.4f}")
print(f"Recall (at best threshold): {recall_score(y_test, y_pred):.4f}")


