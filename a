from pyspark.sql import SparkSession
from pyspark.sql.functions import input_file_name, regexp_extract

# Initialize Spark session
spark = SparkSession.builder.appName("DailyFileVolumeReport").getOrCreate()

# Base path to common_filter directory
base_path = "/Volumes/dbc_adv_anlaytics_dev/surveyspeechextraction/call_intent_workspace/common_filter"

# Load all files under common_filter recursively
files_df = spark.read.format("binaryFile") \
    .option("recursiveFileLookup", "true") \
    .load(base_path)

# Keep only files in 'to_process' folders
filtered_df = files_df.filter(files_df.path.contains("/to_process/"))

# Extract LOB, date, and vendor from path
pattern = r".*/common_filter/([^/]+)/([^/]+)/to_process/([^/]+)/.*"

parsed_df = filtered_df \
    .withColumn("LOB", regexp_extract("path", pattern, 1)) \
    .withColumn("Date", regexp_extract("path", pattern, 2)) \
    .withColumn("Vendor", regexp_extract("path", pattern, 3))

# Group and count files by LOB, Date, and Vendor
result_df = parsed_df.groupBy("LOB", "Date", "Vendor").count() \
                     .withColumnRenamed("count", "FileCount") \
                     .orderBy("LOB", "Date", "Vendor")

# Display the result
display(result_df)


from pyspark.sql.functions import input_file_name, regexp_extract
import os

vendors = ["Broker", "Core Premier", "IFP", "MA", "Medical", "Provider", "Small Group"]
base_path = "/Volumes/dbc_adv_anlaytics_dev/surveyspeechextraction/call_intent_workspace/common_filter/"

df_list = []

for vendor in vendors:
    folder_path = os.path.join(base_path, vendor, "date", "to_process", "*")
    try:
        df = spark.read.format("binaryFile").load(folder_path)
        df = df.withColumn("vendor", regexp_extract("path", r"/common_filter/([^/]+)/", 1)) \
               .withColumn("date", regexp_extract("path", r"/to_process/(\d{4}-\d{2}-\d{2})/", 1))
        df_list.append(df)
    except Exception as e:
        print(f"Skipping {vendor} due to error: {e}")

# Combine and summarize
if df_list:
    combined_df = df_list[0]
    for df in df_list[1:]:
        combined_df = combined_df.union(df)

    result = combined_df.groupBy("vendor", "date").count().orderBy("vendor", "date")
    display(result)
else:
    print("No files found in any vendor folders.")


from pyspark.sql.functions import input_file_name, regexp_extract
import os

vendors = ["Broker", "Core Premier", "IFP", "MA", "Medical", "Provider", "Small Group"]
base_path = "/Volumes/dbc_adv_anlaytics_dev/surveyspeechextraction/call_intent_workspace/common_filter/"

df_list = []

for vendor in vendors:
    full_path = os.path.join(base_path, vendor, "date", "to_process", "*")
    try:
        df = spark.read.format("binaryFile").load(full_path)
        df = df.withColumn("vendor", regexp_extract("path", r"/common_filter/([^/]+)/", 1)) \
               .withColumn("date", regexp_extract("path", r"/date/to_process/(\d{4}-\d{2}-\d{2})/", 1))
        df_list.append(df)
    except Exception as e:
        print(f"Skipping {vendor}: {e}")

# Combine all vendors' data
if df_list:
    combined_df = df_list[0]
    for df in df_list[1:]:
        combined_df = combined_df.union(df)

    # Count files by vendor and date
    result = combined_df.groupBy("vendor", "date").count().orderBy("vendor", "date")
    display(result)
else:
    print("No files found in any LOB folders.")


from pyspark.sql.functions import input_file_name, regexp_extract
import os

# Define your base folder and target subfolders
base_path = "/Volumes/dbc_adv_anlaytics_dev/surveyspeechextraction/call_intent_workspace/common_filter/"
folders = ["Broker", "Core Premier", "IFP", "MA", "Medical", "Provider", "Small Group"]

# Collect data from all folders
df_list = []
for folder in folders:
    full_path = os.path.join(base_path, folder)
    df = spark.read.text(full_path)
    df = df.withColumn("file_path", input_file_name()) \
           .withColumn("vendor", regexp_extract("file_path", r"/([^/]+)/[^/]+$", 1)) \
           .withColumn("date", regexp_extract("file_path", r"/(\d{4}-\d{2}-\d{2})/", 1))
    df_list.append(df)

# Union all
combined_df = df_list[0]
for df in df_list[1:]:
    combined_df = combined_df.union(df)

# Group by date and vendor and count
result = combined_df.groupBy("vendor", "date").count().orderBy("date", "vendor")
display(result)



from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your new Verint path
verint_path = "/Volumes/prod_adb/default/wst-data-volume-stmlz/surveyspeechextraction/Call_Transcript/2025_verint_transcripts/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read all files recursively
verint_df = spark.read.format("binaryfile") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(verint_path)

# Extract the date folder from path
# Example: '/.../2025-01-01/file.wav' → '2025-01-01'
verint_df = verint_df.withColumn("date_folder", F.element_at(F.split("path", "/"), -2))

# Extract year-month from date_folder
verint_df = verint_df.withColumn("month", F.substring("date_folder", 1, 7))

# Group by month and sum file size
verint_space_by_month = verint_df.groupBy("month").agg(F.sum("length").alias("total_bytes"))

# Convert to MB
verint_space_by_month = verint_space_by_month.withColumn("total_mb", F.round(verint_space_by_month["total_bytes"] / (1024 * 1024), 2))

# Order nicely
verint_space_by_month = verint_space_by_month.orderBy("month")

# Display
display(verint_space_by_month)




from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files recursively
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(path)

# Extract filename and folder
df = df.withColumn("filename", F.element_at(F.split("path", "/"), -1)) \
       .withColumn("folder", F.element_at(F.split("path", "/"), -2))

# Extract year-month from filename
# First try "YYYY-MM" format
df = df.withColumn("month_dash", F.regexp_extract("filename", r"(\d{4}-\d{2})", 1))

# Then try "YYYYMM" format (TTEC files)
df = df.withColumn("month_nodash", F.regexp_extract("filename", r"(\d{6})", 1))

# Choose month column
df = df.withColumn(
    "month",
    F.when(F.col("month_dash") != "", F.col("month_dash")) \
     .when(F.col("month_nodash") != "", 
           F.concat_ws("-", F.col("month_nodash").substr(1,4), F.col("month_nodash").substr(5,2))) \
     .otherwise(None)
)

# Remove rows with null month
df = df.filter(F.col("month").isNotNull())

# Group by folder and month, sum length
space_by_folder_month = df.groupBy("folder", "month").agg(F.sum("length").alias("total_bytes"))

# Convert bytes to MB
space_by_folder_month = space_by_folder_month.withColumn("total_mb", F.round(space_by_folder_month["total_bytes"] / (1024 * 1024), 2))

# Order nicely
space_by_folder_month = space_by_folder_month.orderBy("folder", "month")

# (Optional) Calculate total per folder
folder_totals = space_by_folder_month.groupBy("folder").agg(F.round(F.sum("total_mb"),2).alias("folder_total_mb"))

# Display results
display(space_by_folder_month)
display(folder_totals)




from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files recursively
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(path)

# Extract filename and folder
df = df.withColumn("filename", F.element_at(F.split("path", "/"), -1)) \
       .withColumn("folder", F.element_at(F.split("path", "/"), -2))

# Extract year-month from filename
df = df.withColumn("month", F.regexp_extract("filename", r"(\d{4}-\d{2})", 1))

# ----- Remove rows with empty month -----
df = df.filter(F.col("month") != "")

# Group by folder and month, sum length
space_by_folder_month = df.groupBy("folder", "month").agg(F.sum("length").alias("total_bytes"))

# Convert to MB
space_by_folder_month = space_by_folder_month.withColumn("total_mb", F.round(space_by_folder_month["total_bytes"] / (1024 * 1024), 2))

# Order nicely
space_by_folder_month = space_by_folder_month.orderBy("folder", "month")

# (Optional) Calculate total per folder
folder_totals = space_by_folder_month.groupBy("folder").agg(F.sum("total_mb").alias("folder_total_mb"))

# Display
display(space_by_folder_month)
display(folder_totals)



from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files recursively
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(path)

# Extract filename
df = df.withColumn("filename", F.element_at(F.split("path", "/"), -1))

# Extract year-month from filename
df = df.withColumn("month", F.regexp_extract("filename", r"(\d{4}-\d{2})", 1))

# Group by month and sum
space_by_month = df.groupBy("month").agg(F.sum("length").alias("total_bytes"))

# Convert bytes to MB
space_by_month = space_by_month.withColumn("total_mb", F.round(space_by_month["total_bytes"] / (1024 * 1024), 2))

# Keep only necessary columns
space_by_month = space_by_month.select("month", "total_mb").orderBy("month")

# ----- Add total row -----
# 1. Sum total
total_sum = space_by_month.agg(F.sum("total_mb")).collect()[0][0]

# 2. Create total row manually
from pyspark.sql import Row
total_row = spark.createDataFrame([Row(month="Total", total_mb=round(total_sum, 2))])

# 3. Union original + total row
final_result = space_by_month.unionByName(total_row)

# Display
display(final_result)



from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files recursively
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(path)

# Extract filename
df = df.withColumn("filename", F.element_at(F.split("path", "/"), -1))

# Extract year-month from filename
df = df.withColumn("month", F.regexp_extract("filename", r"(\d{4}-\d{2})", 1))

# Group by month and sum
space_by_month = df.groupBy("month").agg(F.sum("length").alias("total_bytes"))

# Convert bytes to MB
space_by_month = space_by_month.withColumn("total_mb", F.round(space_by_month["total_bytes"] / (1024 * 1024), 2))

# Optionally convert to GB if preferred
space_by_month = space_by_month.withColumn("total_gb", F.round(space_by_month["total_bytes"] / (1024 * 1024 * 1024), 2))

# Select final columns you want to display: MB only or GB only
# If you want MB only:
final_result_mb = space_by_month.select("month", "total_mb")

# If you want GB only:
final_result_gb = space_by_month.select("month", "total_gb")

# Display (choose one)
display(final_result_mb)
# display(final_result_gb)   # uncomment if you want GB instead


from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files with recursive lookup
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \   # <-- very important!!
    .schema(schema) \
    .load(path)

# Extract folder name (2nd last part of path)
df = df.withColumn("folder", F.element_at(F.split("path", "/"), -2))

# Group by folder and sum sizes
space_by_folder = df.groupBy("folder").agg(F.sum("length").alias("total_bytes"))

# Optional: Convert to MB
space_by_folder = space_by_folder.withColumn("total_mb", F.round(space_by_folder["total_bytes"] / (1024 * 1024), 2))

# Display
display(space_by_folder)


from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files with schema
df = spark.read.format("binaryfile").option("pathGlobFilter", "*.zip").schema(schema).load(path)

# Extract FOLDER name (not file name)
# Example: /Volumes/prod_adb/.../Sagility/xxx.zip → 'Sagility'
df = df.withColumn("folder", F.element_at(F.split("path", "/"), -2))

# Group by folder and sum size
space_by_folder = df.groupBy("folder").agg(F.sum("length").alias("total_bytes"))

# Optional: Convert bytes to MB
space_by_folder = space_by_folder.withColumn("total_mb", F.round(space_by_folder["total_bytes"] / (1024 * 1024), 2))

# Display
display(space_by_folder)