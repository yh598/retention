from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files recursively
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(path)

# Extract filename
df = df.withColumn("filename", F.element_at(F.split("path", "/"), -1))

# Extract year-month from filename
df = df.withColumn("month", F.regexp_extract("filename", r"(\d{4}-\d{2})", 1))

# Group by month and sum
space_by_month = df.groupBy("month").agg(F.sum("length").alias("total_bytes"))

# Convert bytes to MB
space_by_month = space_by_month.withColumn("total_mb", F.round(space_by_month["total_bytes"] / (1024 * 1024), 2))

# Keep only necessary columns
space_by_month = space_by_month.select("month", "total_mb").orderBy("month")

# ----- Add total row -----
# 1. Sum total
total_sum = space_by_month.agg(F.sum("total_mb")).collect()[0][0]

# 2. Create total row manually
from pyspark.sql import Row
total_row = spark.createDataFrame([Row(month="Total", total_mb=round(total_sum, 2))])

# 3. Union original + total row
final_result = space_by_month.unionByName(total_row)

# Display
display(final_result)



from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files recursively
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \
    .schema(schema) \
    .load(path)

# Extract filename
df = df.withColumn("filename", F.element_at(F.split("path", "/"), -1))

# Extract year-month from filename
df = df.withColumn("month", F.regexp_extract("filename", r"(\d{4}-\d{2})", 1))

# Group by month and sum
space_by_month = df.groupBy("month").agg(F.sum("length").alias("total_bytes"))

# Convert bytes to MB
space_by_month = space_by_month.withColumn("total_mb", F.round(space_by_month["total_bytes"] / (1024 * 1024), 2))

# Optionally convert to GB if preferred
space_by_month = space_by_month.withColumn("total_gb", F.round(space_by_month["total_bytes"] / (1024 * 1024 * 1024), 2))

# Select final columns you want to display: MB only or GB only
# If you want MB only:
final_result_mb = space_by_month.select("month", "total_mb")

# If you want GB only:
final_result_gb = space_by_month.select("month", "total_gb")

# Display (choose one)
display(final_result_mb)
# display(final_result_gb)   # uncomment if you want GB instead


from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files with recursive lookup
df = spark.read.format("binaryfile") \
    .option("pathGlobFilter", "*.zip") \
    .option("recursiveFileLookup", "true") \   # <-- very important!!
    .schema(schema) \
    .load(path)

# Extract folder name (2nd last part of path)
df = df.withColumn("folder", F.element_at(F.split("path", "/"), -2))

# Group by folder and sum sizes
space_by_folder = df.groupBy("folder").agg(F.sum("length").alias("total_bytes"))

# Optional: Convert to MB
space_by_folder = space_by_folder.withColumn("total_mb", F.round(space_by_folder["total_bytes"] / (1024 * 1024), 2))

# Display
display(space_by_folder)


from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BinaryType
from pyspark.sql import functions as F

# Set your path
path = "/Volumes/prod_adb/default/synapse_volume/raw/surveyspeechextraction/"

# Define schema
schema = StructType([
    StructField("path", StringType(), False),
    StructField("modificationTime", TimestampType(), False),
    StructField("length", LongType(), False),
    StructField("content", BinaryType(), True)
])

# Read files with schema
df = spark.read.format("binaryfile").option("pathGlobFilter", "*.zip").schema(schema).load(path)

# Extract FOLDER name (not file name)
# Example: /Volumes/prod_adb/.../Sagility/xxx.zip â†’ 'Sagility'
df = df.withColumn("folder", F.element_at(F.split("path", "/"), -2))

# Group by folder and sum size
space_by_folder = df.groupBy("folder").agg(F.sum("length").alias("total_bytes"))

# Optional: Convert bytes to MB
space_by_folder = space_by_folder.withColumn("total_mb", F.round(space_by_folder["total_bytes"] / (1024 * 1024), 2))

# Display
display(space_by_folder)