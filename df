from collections import Counter
import pyspark.sql.functions as F

# Check the Shape of the DataFrame
num_rows = df.count()
num_cols = len(df.columns)
print(f"Original Shape of DataFrame: ({num_rows}, {num_cols})")

# Identify Duplicate Columns
column_counts = Counter(df.columns)
duplicate_columns = [col for col, count in column_counts.items() if count > 1]

# Find actual names of latitude/longitude columns
lat_cols = [col for col in df.columns if "LATITUDE" in col]
lon_cols = [col for col in df.columns if "LONGITUDE" in col]

print(f"Duplicated Columns (excluding LATITUDE & LONGITUDE): {duplicate_columns}")
print(f"Latitude Columns: {lat_cols}")
print(f"Longitude Columns: {lon_cols}")

# Merge Latitude and Longitude (Keep First Non-Null Value)
if len(lat_cols) > 1:
    df = df.withColumn("LATITUDE", F.coalesce(*[F.col(col) for col in lat_cols]))

if len(lon_cols) > 1:
    df = df.withColumn("LONGITUDE", F.coalesce(*[F.col(col) for col in lon_cols]))

# Drop Redundant Duplicate Columns
columns_to_drop = [col for col in duplicate_columns if col not in ["LATITUDE", "LONGITUDE"]]
columns_to_drop += lat_cols + lon_cols  # Drop extra latitude/longitude columns after merging

df = df.drop(*columns_to_drop)

# Check the New Shape After Removing Duplicates
num_rows = df.count()
num_cols = len(df.columns)
print(f"Final Shape after merging and cleaning: ({num_rows}, {num_cols})")