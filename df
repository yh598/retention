from collections import Counter
import pyspark.sql.functions as F

# Check the Shape of the DataFrame
num_rows = df.count()
num_cols = len(df.columns)
print(f"Original Shape of DataFrame: ({num_rows}, {num_cols})")

# Identify Duplicate Columns
column_counts = Counter(df.columns)
duplicate_columns = [col for col, count in column_counts.items() if count > 1]

# Find actual names of latitude/longitude columns
lat_cols = [col for col in df.columns if "LATITUDE" in col]
lon_cols = [col for col in df.columns if "LONGITUDE" in col]

print(f"Duplicated Columns (excluding LATITUDE & LONGITUDE): {duplicate_columns}")
print(f"Latitude Columns: {lat_cols}")
print(f"Longitude Columns: {lon_cols}")

# Merge Latitude and Longitude (Keep First Non-Null Value)
if len(lat_cols) > 1:
    df = df.withColumn("LATITUDE", F.coalesce(*[F.col(col) for col in lat_cols]))

if len(lon_cols) > 1:
    df = df.withColumn("LONGITUDE", F.coalesce(*[F.col(col) for col in lon_cols]))

# Drop Redundant Duplicate Columns
columns_to_drop = [col for col in duplicate_columns if col not in ["LATITUDE", "LONGITUDE"]]
columns_to_drop += lat_cols + lon_cols  # Drop extra latitude/longitude columns after merging

df = df.drop(*columns_to_drop)

# Check the New Shape After Removing Duplicates
num_rows = df.count()
num_cols = len(df.columns)
print(f"Final Shape after merging and cleaning: ({num_rows}, {num_cols})")

import pyspark.sql.functions as F

# Add provider_flag column (1 if PROV_IPA_ID is not null, else 0)
df = df.withColumn("provider_flag", F.when(F.col("PROV_IPA_ID").isNotNull(), 1).otherwise(0))

import pyspark.sql.functions as F
from math import radians, sin, cos, sqrt, atan2

# Define Haversine formula as a UDF (distance in miles)
def haversine_miles(lat1, lon1, lat2, lon2):
    if lat1 is None or lon1 is None or lat2 is None or lon2 is None:
        return None  # Return None if any coordinate is missing

    R = 3958.8  # Earth radius in miles

    # Convert degrees to radians
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])

    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))

    return R * c  # Distance in miles

# Register as a PySpark UDF
haversine_miles_udf = F.udf(haversine_miles)

# Calculate distance only if provider_flag = 1
df = df.withColumn(
    "provider_to_home_distance",
    F.when(F.col("provider_flag") == 1,
           haversine_miles_udf(F.col("LATITUDE"), F.col("LONGITUDE"),
                               F.col("HOME_LATITUDE"), F.col("HOME_LONGITUDE")))
    .otherwise(None)  # Assign None if provider_flag != 1
)

from collections import Counter

# Get column types
column_types = [dtype for col_name, dtype in df.dtypes]

# Count occurrences of each data type
type_counts = Counter(column_types)

# Display the count of each type
print("Column Count by Data Type:")
for dtype, count in type_counts.items():
    print(f"{dtype}: {count}")

from pyspark.sql.functions import col

# Identify columns where all values are null or NaN
columns_to_drop = [col_name for col_name in df.columns if df.select(col_name).filter(col(col_name).isNotNull()).count() == 0]

# Drop identified columns
df = df.drop(*columns_to_drop)

# Print confirmation
print(f"Dropped {len(columns_to_drop)} columns that contained only null or NaN values.")
