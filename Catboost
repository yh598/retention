# ==============================
# ðŸ“Œ Step 1: Load Required Libraries
# ==============================
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, isnan, year, month
from pyspark.ml.feature import StringIndexer

from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

# Initialize Spark Session
spark = SparkSession.builder.appName("RetentionPrediction").getOrCreate()

# ==============================
# ðŸ“Œ Step 2: Load Parquet Data
# ==============================
file_path = "your_dataset.parquet"  # Replace with actual file path
df = spark.read.parquet(file_path)

df.printSchema()
df.show(5)

# ==============================
# ðŸ“Œ Step 3: Drop Unnecessary Columns
# ==============================
drop_columns = [col for col in df.columns if "_HASH" in col or 
                "_ID" in col or 
                "NAME" in col or 
                "PHONE" in col or 
                "EMAIL" in col or 
                "ADDRESS" in col or 
                "FIPS" in col or 
                "TRACTCODE" in col or 
                "CENSUS" in col]

df = df.drop(*drop_columns)

print("Dropped columns:", drop_columns)
df.printSchema()

# ==============================
# ðŸ“Œ Step 4: Handle Missing Values
# ==============================
numeric_cols = [c for c, t in df.dtypes if t in ("int", "double")]
for col_name in numeric_cols:
    median_value = df.approxQuantile(col_name, [0.5], 0.01)[0]
    df = df.fillna({col_name: median_value})

categorical_cols = [c for c, t in df.dtypes if t == "string"]
for col_name in categorical_cols:
    mode_value = df.groupBy(col_name).count().orderBy(col("count").desc()).first()[0]
    df = df.fillna({col_name: mode_value})

# ==============================
# ðŸ“Œ Step 5: Feature Engineering
# ==============================
indexers = [StringIndexer(inputCol=col_name, outputCol=f"{col_name}_index").fit(df) for col_name in categorical_cols]
for indexer in indexers:
    df = indexer.transform(df)

df = df.withColumn("study_start_year", year(col("STUDY_START_DT")))
df = df.withColumn("study_start_month", month(col("STUDY_START_DT")))
df = df.drop(*categorical_cols, "STUDY_START_DT")

df.show(5)

# ==============================
# ðŸ“Œ Step 6: Convert Spark DataFrame to Pandas
# ==============================
df_pandas = df.toPandas()

target = "survival_flag"
features = [c for c in df_pandas.columns if c != target]
cat_features = [col for col in df_pandas.columns if "_index" in col]

# ==============================
# ðŸ“Œ Step 7: Handle Class Imbalance
# ==============================
retention_rate = df_pandas[target].mean()
print(f"Retention Rate: {retention_rate:.2%}")

X_train, X_test, y_train, y_test = train_test_split(df_pandas[features], df_pandas[target], test_size=0.2, random_state=42)

if retention_rate > 0.80:
    total_samples = len(y_train)
    class_0_weight = total_samples / (2 * sum(y_train == 0))
    class_1_weight = total_samples / (2 * sum(y_train == 1))
    class_weights = [class_0_weight, class_1_weight]

    undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)
    X_train, y_train = undersampler.fit_resample(X_train, y_train)

    smote = SMOTE(sampling_strategy=0.5, random_state=42)
    X_train, y_train = smote.fit_resample(X_train, y_train)
else:
    class_weights = None

# ==============================
# ðŸ“Œ Step 8: Train CatBoost Model
# ==============================
train_pool = Pool(X_train, label=y_train, cat_features=cat_features)
test_pool = Pool(X_test, label=y_test, cat_features=cat_features)

cat_model = CatBoostClassifier(
    iterations=500, depth=8, learning_rate=0.05, loss_function="Logloss",
    eval_metric="AUC", verbose=100, class_weights=class_weights
)

cat_model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=50)

# ==============================
# ðŸ“Œ Step 9: Evaluate Model Performance
# ==============================
y_pred = cat_model.predict(X_test)
y_pred_proba = cat_model.predict_proba(X_test)[:, 1]

print("Accuracy:", accuracy_score(y_test, y_pred))
print("AUC-ROC Score:", roc_auc_score(y_test, y_pred_proba))
print(classification_report(y_test, y_pred))

# ==============================
# ðŸ“Œ Step 10: Save Model & Predict on New Data
# ==============================
cat_model.save_model("catboost_retention_model.cbm")
print("Model saved successfully!")

# Load new dataset
new_data = spark.read.parquet("new_data.parquet")
for indexer in indexers:
    new_data = indexer.transform(new_data)

new_data = new_data.withColumn("study_start_year", year(col("STUDY_START_DT")))
new_data = new_data.withColumn("study_start_month", month(col("STUDY_START_DT")))
new_data = new_data.drop(*categorical_cols, "STUDY_START_DT")

new_data_pandas = new_data.toPandas()
new_predictions = cat_model.predict(new_data_pandas[features])

import pandas as pd
import catboost
from catboost import CatBoostClassifier
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler

# Convert Spark DataFrame to Pandas
pandas_df = sql_df.toPandas()

# Define Features and Target Variable
target_col = "survival_flag"  # Assuming 'survival_flag' is the target column
feature_cols = [col for col in pandas_df.columns if col != target_col]

X = pandas_df[feature_cols]
y = pandas_df[target_col]

# Convert categorical columns to string (if needed for CatBoost)
categorical_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()

# Initialize and Train CatBoost Classifier
catboost_model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, verbose=0)
catboost_model.fit(X, y, cat_features=categorical_cols)

# Get Feature Importances
feature_importance = catboost_model.get_feature_importance()
feature_importance_df = pd.DataFrame({"Feature": feature_cols, "Importance": feature_importance})

# Sort by importance
feature_importance_df = feature_importance_df.sort_values(by="Importance", ascending=False)

# Show top 10 features
import ace_tools as tools
tools.display_dataframe_to_user(name="Feature Importance", dataframe=feature_importance_df)
print(new_predictions)
from pyspark.ml.feature import Imputer
from pyspark.sql.functions import col

# âœ… Identify Numeric and Categorical Columns
numeric_cols = [col_name for col_name, dtype in df1.dtypes if dtype in ('int', 'double') or dtype.startswith('decimal')]
categorical_cols = [col_name for col_name, dtype in df1.dtypes if dtype == 'string']

# âœ… Convert Decimal Columns to Double for Imputation
for col_name, dtype in df1.dtypes:
    if dtype.startswith("decimal"):
        df1 = df1.withColumn(col_name, col(col_name).cast("double"))

# âœ… Handle Missing Values for Numerical Columns WITHOUT Renaming Columns
num_imputer = Imputer(strategy="median", inputCols=numeric_cols, outputCols=numeric_cols)  # Keep original names
df2 = num_imputer.fit(df1).transform(df1)

# âœ… Handle Missing Values for Categorical Columns WITHOUT Encoding (CatBoost can handle them as-is)
df2 = df2.fillna({col_name: "missing" for col_name in categorical_cols})

# âœ… Check Missing Values After Handling
df2.select([(col(c).isNull().cast("int")).alias(c) for c in df2.columns]).show()

import pandas as pd
import numpy as np
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc

# Define target variable and feature columns
target_col = "churn_flag_imputed"
feature_cols = [col for col in pandas_df.columns if col != target_col]

X = pandas_df[feature_cols]
y = pandas_df[target_col]

# Identify categorical columns
categorical_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()

# Convert categorical variables to strings (CatBoost handles them natively)
for col in categorical_cols:
    X[col] = X[col].astype(str)

# Train-Test Split (Maintain Class Distribution)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Compute class weights to handle imbalance
class_weights = {
    0: len(y_train) / (2 * (y_train == 0).sum()),
    1: len(y_train) / (2 * (y_train == 1).sum())
}

# Use Pool object for CatBoost
train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_cols)
test_pool = Pool(data=X_test, label=y_test, cat_features=categorical_cols)

# Initialize and train CatBoost classifier with class weights
catboost_model = CatBoostClassifier(
    iterations=500, depth=6, learning_rate=0.1, verbose=100,
    class_weights=[class_weights[0], class_weights[1]], loss_function="Logloss"
)

catboost_model.fit(train_pool)

# Get predictions
y_pred_prob = catboost_model.predict_proba(test_pool)[:, 1]
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluate model performance
roc_auc = roc_auc_score(y_test, y_pred_prob)
precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
pr_auc = auc(recall, precision)

print("ROC AUC Score:", round(roc_auc, 4))
print("Precision-Recall AUC:", round(pr_auc, 4))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Get feature importance
feature_importance = catboost_model.get_feature_importance()
feature_importance_df = pd.DataFrame({"Feature": feature_cols, "Importance": feature_importance})

# Sort by importance and display top features
feature_importance_df = feature_importance_df.sort_values(by="Importance", ascending=False)
print("Most Important Features:\n", feature_importance_df.head(10))

# Display feature importance table
import ace_tools as tools
tools.display_dataframe_to_user(name="Feature Importance", dataframe=feature_importance_df)
