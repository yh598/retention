from pyspark.sql.functions import col
from pyspark.sql import SparkSession

# Assuming `sql_df` is the Spark DataFrame from your query
survival_1_count = sql_df.filter(col("survival_flag") == 1).count()
survival_0_df = sql_df.filter(col("survival_flag") == 0)
survival_1_df = sql_df.filter(col("survival_flag") == 1)

# Downsample the majority class (survival_flag = 0) to match survival_flag = 1 count
survival_0_downsampled = survival_0_df.sample(False, fraction=survival_1_count / survival_0_df.count(), seed=42)

# Combine both subsets
balanced_df = survival_0_downsampled.union(survival_1_df)

# Show results
balanced_df.groupBy("survival_flag").count().show()
